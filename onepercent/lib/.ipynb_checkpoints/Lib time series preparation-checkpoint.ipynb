{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named wekeypedia.wikipedia.page",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-dc3dfc7db791>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mIPython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdisplay\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mHTML\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mwekeypedia\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwikipedia\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpage\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mWikipediaPage\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mPage\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m: No module named wekeypedia.wikipedia.page"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_formats=['svg']\n",
    "\n",
    "import json\n",
    "import os\n",
    "import hashlib\n",
    "import sys\n",
    "import math\n",
    "import codecs\n",
    "\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as dates\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import networkx as nx\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "from wekeypedia.wikipedia.page import WikipediaPage as Page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Data gathering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_path_to_file(name):\n",
    "    hashmd5 = hashlib.md5(name.encode('utf-8')).hexdigest()\n",
    "    path = '/'+hashmd5[0]+'/'+hashmd5[1]+'/'+hashmd5[2]+'/'+hashmd5[3]+'/'\n",
    "    return path,hashmd5\n",
    "\n",
    "def load_pages_data(dataset_dir_name, list_of_page_names):\n",
    "    data_pages_dir_name = '%s/data/pages/' % dataset_dir_name\n",
    "    if not(os.path.exists(data_pages_dir_name)): os.makedirs(data_pages_dir_name)\n",
    "\n",
    "    pages_data = {}\n",
    "    for page_name in list_of_page_names:\n",
    "        path,hashmd5 = get_path_to_file(page_name)\n",
    "        if not(os.path.exists( '%s/%s' % (data_pages_dir_name, path) )):\n",
    "            os.makedirs( '%s/%s' % (data_pages_dir_name, path) )\n",
    "        file_name = '%s/%s/%s.json'%(data_pages_dir_name, path, hashmd5)\n",
    "        if (os.path.exists(file_name)):\n",
    "            with open(file_name) as f:\n",
    "                pages_data[page_name] = json.load(f)\n",
    "        else:\n",
    "            pass\n",
    "    return pages_data\n",
    "\n",
    "\n",
    "def gather_pages_data(dataset_dir_name, list_of_page_names):\n",
    "    data_pages_dir_name = '%s/data/pages/' % dataset_dir_name\n",
    "    if not(os.path.exists(data_pages_dir_name)): os.makedirs(data_pages_dir_name)\n",
    "\n",
    "    pages_data = {}\n",
    "    for page_name in list_of_page_names:\n",
    "        path,hashmd5 = get_path_to_file(page_name)\n",
    "        if not(os.path.exists( '%s/%s' % (data_pages_dir_name, path) )):\n",
    "            os.makedirs( '%s/%s' % (data_pages_dir_name, path) )\n",
    "        file_name = '%s/%s/%s.json'%(data_pages_dir_name, path, hashmd5)\n",
    "        if (os.path.exists(file_name)):\n",
    "            with open(file_name) as f:\n",
    "                pages_data[page_name] = json.load(f)\n",
    "        else:\n",
    "            data = {}\n",
    "            wikipage = Page(title=page_name)\n",
    "            request = wikipage.fetch_info(page_name)['query']['pages']\n",
    "            page_id = list(request)[0]\n",
    "            if page_id!='-1':\n",
    "                try:\n",
    "                    for x in request[page_id]:\n",
    "                        data[x]=request[page_id][x]\n",
    "                    data['revisions']=wikipage.get_revisions_list()\n",
    "                    data['links']= wikipage.get_links()\n",
    "                    data['backlinks']= wikipage.get_backlinks()\n",
    "                    data['categories']= wikipage.get_categories()\n",
    "                    pages_data[page_name]=data\n",
    "                    f = open(file_name,'w')\n",
    "                    f.write(json.dumps(data))\n",
    "                    f.close()\n",
    "                except Exception as e:\n",
    "                    print 'Error with page:',page_name\n",
    "                    print e\n",
    "    return(pages_data)\n",
    "\n",
    "\n",
    "def gather_pages_views(dataset_dir_name,list_of_page_names):\n",
    "    pages_views_dir_name = '%s/data/pagesviews/' % dataset_dir_name\n",
    "    if not(os.path.exists(pages_views_dir_name)): os.makedirs(pages_views_dir_name)\n",
    "\n",
    "    error = []\n",
    "    for page_name in list_of_page_names:\n",
    "        path,hashmd5 = get_path_to_file(page_name)\n",
    "        if not(os.path.exists( '%s/%s' % (pages_views_dir_name,path) )):\n",
    "            os.makedirs( '%s/%s' % (pages_views_dir_name,path) )\n",
    "        file_name = '%s/%s/%s.json' % (pages_views_dir_name,path, hashmd5)        \n",
    "        if not (os.path.exists(file_name)):            \n",
    "            try:\n",
    "                wikipage = Page(title=page_name)\n",
    "                page_views_ts = {k:v for x in wikipage.get_pageviews() for (k,v) in x.items()}\n",
    "                f = open(file_name,'w')\n",
    "                f.write(json.dumps(page_views_ts))\n",
    "                f.close()\n",
    "            except:\n",
    "                error.append(page_name)\n",
    "    return(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#get quality\n",
    "def get_quality(page):\n",
    "    talk_name = 'Talk:'+page\n",
    "    talk = gather_pages_data(dataset_dir_name, [ talk_name ])\n",
    "    if len(talk)==0: return 0,0,0\n",
    "    quality=1\n",
    "    impor=0\n",
    "    islist=0\n",
    "    for c in talk[talk_name]['categories']:\n",
    "        cat = c['title'].lower()\n",
    "        if '-class' in cat:\n",
    "            if 'fa-class' in cat or 'fl-class' in cat: quality = max(quality,6)\n",
    "            if 'a-class' in cat or 'ga-class' in cat : quality = max(quality,5)\n",
    "            #if 'ga-class' in cat: quality = max(quality,5)\n",
    "            if 'b-class' in cat and 'stub-class' not in cat: quality = max(quality,4)\n",
    "            if 'c-class' in cat: quality = max(quality,3)\n",
    "            if 'start-class' in cat: quality = max(quality,2)\n",
    "            if 'stub-class' in cat: quality = max(quality,1)\n",
    "            if 'fl-class' in cat or 'list-class' in cat: islist=1\n",
    "        if 'importance' in cat:\n",
    "            if 'top' in cat: impor=max(impor,4)\n",
    "            if 'high' in cat: impor=max(impor,3)\n",
    "            if 'mid' in cat: impor=max(impor,2)\n",
    "            if 'low' in cat: impor=max(impor,1)            \n",
    "    return (quality,impor,islist)\n",
    "\n",
    "def stat_computation(dataset_dir_name,list_of_page_names):\n",
    "    df = pd.DataFrame(index=list_of_page_names)\n",
    "    for  page in list_of_page_names:\n",
    "        pages_data=gather_pages_data(dataset_dir_name, [page])\n",
    "        if len(pages_data)>0:\n",
    "            #length\n",
    "            data={k:v['length'] for k,v in pages_data.items()}\n",
    "            for k in pages_data.keys(): df.ix[k,'Length'] = data[k]\n",
    "\n",
    "            #nombre de revisions\n",
    "            a,b,c,data = get_pages_revisions_time_series(dataset_dir_name,pages_data.keys() )\n",
    "            for k in data.keys(): df.ix[k,'revisions_2001'] = data[k]['2000-12-31':'2001-12-31'].sum()['revisions']\n",
    "            #number of revisions in 2011\n",
    "            for k in data.keys(): df.ix[k,'revisions_2002'] = data[k]['2001-12-31':'2002-12-31'].sum()['revisions']\n",
    "            #number of revisions in 2012\n",
    "            for k in data.keys(): df.ix[k,'revisions_2003'] = data[k]['2002-12-31':'2003-12-31'].sum()['revisions']\n",
    "            #number of revisions in 2013\n",
    "            for k in data.keys(): df.ix[k,'revisions_2004'] = data[k]['2003-12-31':'2004-12-31'].sum()['revisions']\n",
    "            #number of revisions in 2014\n",
    "            for k in data.keys(): df.ix[k,'revisions_2005'] = data[k]['2004-12-31':'2005-12-31'].sum()['revisions']\n",
    "            #number of revisions in 2011\n",
    "            for k in data.keys(): df.ix[k,'revisions_2006'] = data[k]['2005-12-31':'2006-12-31'].sum()['revisions']\n",
    "            #number of revisions in 2012\n",
    "            for k in data.keys(): df.ix[k,'revisions_2007'] = data[k]['2006-12-31':'2007-12-31'].sum()['revisions']\n",
    "            #number of revisions in 2013\n",
    "            for k in data.keys(): df.ix[k,'revisions_2008'] = data[k]['2007-12-31':'2008-12-31'].sum()['revisions']\n",
    "            #number of revisions in 2013\n",
    "            for k in data.keys(): df.ix[k,'revisions_2009'] = data[k]['2008-12-31':'2009-12-31'].sum()['revisions']\n",
    "            #number of revisions in 2014\n",
    "            for k in data.keys(): df.ix[k,'revisions_2010'] = data[k]['2009-12-31':'2010-12-31'].sum()['revisions']\n",
    "            #number of revisions in 2011\n",
    "            for k in data.keys(): df.ix[k,'revisions_2011'] = data[k]['2010-12-31':'2011-12-31'].sum()['revisions']\n",
    "            #number of revisions in 2012\n",
    "            for k in data.keys(): df.ix[k,'revisions_2012'] = data[k]['2011-12-31':'2012-12-31'].sum()['revisions']\n",
    "            #number of revisions in 2013\n",
    "            for k in data.keys(): df.ix[k,'revisions_2013'] = data[k]['2012-12-31':'2013-12-31'].sum()['revisions']\n",
    "            #number of revisions in 2014\n",
    "            for k in data.keys(): df.ix[k,'revisions_2014'] = data[k]['2013-12-31':'2014-12-31'].sum()['revisions']\n",
    "            \n",
    "            \n",
    "            #number of revisions before 2010\n",
    "            for k in data.keys(): df.ix[k,'revisions_bef_2002'] = data[k]['1998-12-31':'2001-12-31'].sum()['revisions']\n",
    "            #number of revisions before 2012\n",
    "            for k in data.keys(): df.ix[k,'revisions_bef_2003'] = data[k]['1998-12-31':'2002-12-31'].sum()['revisions']\n",
    "            #number of revisions before 2013\n",
    "            for k in data.keys(): df.ix[k,'revisions_bef_2004'] = data[k]['1998-12-31':'2003-12-31'].sum()['revisions']\n",
    "            #number of revisions before 2014\n",
    "            for k in data.keys(): df.ix[k,'revisions_bef_2005'] = data[k]['1998-12-31':'2004-12-31'].sum()['revisions']\n",
    "            #number of revisions before 2010\n",
    "            for k in data.keys(): df.ix[k,'revisions_bef_2006'] = data[k]['1998-12-31':'2005-12-31'].sum()['revisions']\n",
    "            #number of revisions before 2011\n",
    "            for k in data.keys(): df.ix[k,'revisions_bef_2007'] = data[k]['1998-12-31':'2006-12-31'].sum()['revisions']\n",
    "            #number of revisions before 2012\n",
    "            for k in data.keys(): df.ix[k,'revisions_bef_2008'] = data[k]['1998-12-31':'2007-12-31'].sum()['revisions']\n",
    "            #number of revisions before 2013\n",
    "            for k in data.keys(): df.ix[k,'revisions_bef_2009'] = data[k]['1998-12-31':'2008-12-31'].sum()['revisions']\n",
    "            #number of revisions before 2014\n",
    "            for k in data.keys(): df.ix[k,'revisions_bef_2010'] = data[k]['1998-12-31':'2009-12-31'].sum()['revisions']\n",
    "            #number of revisions before 2010\n",
    "            for k in data.keys(): df.ix[k,'revisions_bef_2011'] = data[k]['1998-12-31':'2010-12-31'].sum()['revisions']\n",
    "            #number of revisions before 2011\n",
    "            for k in data.keys(): df.ix[k,'revisions_bef_2012'] = data[k]['1998-12-31':'2011-12-31'].sum()['revisions']\n",
    "            #number of revisions before 2012\n",
    "            for k in data.keys(): df.ix[k,'revisions_bef_2013'] = data[k]['1998-12-31':'2012-12-31'].sum()['revisions']\n",
    "            #number of revisions before 2013\n",
    "            for k in data.keys(): df.ix[k,'revisions_bef_2014'] = data[k]['1998-12-31':'2013-12-31'].sum()['revisions']\n",
    "            #number of revisions before 2014\n",
    "            for k in data.keys(): df.ix[k,'revisions_bef_2015'] = data[k]['1998-12-31':'2014-12-31'].sum()['revisions']\n",
    "\n",
    "                \n",
    "            #nombre de revisions by members\n",
    "            a,b,c,data = get_pages_members_revisions_time_series(dataset_dir_name,pages_data.keys() )\n",
    "            #number of revisions in 2011\n",
    "            for k in data.keys(): df.ix[k,'revisions_members_2001'] = data[k]['2000-12-31':'2001-12-31'].sum()['revisions']\n",
    "            #number of revisions in 2012\n",
    "            for k in data.keys(): df.ix[k,'revisions_members_2002'] = data[k]['2001-12-31':'2002-12-31'].sum()['revisions']\n",
    "            #number of revisions in 2013\n",
    "            for k in data.keys(): df.ix[k,'revisions_members_2003'] = data[k]['2002-12-31':'2003-12-31'].sum()['revisions']\n",
    "            #number of revisions in 2014\n",
    "            for k in data.keys(): df.ix[k,'revisions_members_2004'] = data[k]['2003-12-31':'2004-12-31'].sum()['revisions']\n",
    "            #number of revisions in 2011\n",
    "            for k in data.keys(): df.ix[k,'revisions_members_2005'] = data[k]['2004-12-31':'2005-12-31'].sum()['revisions']\n",
    "            #number of revisions in 2012\n",
    "            for k in data.keys(): df.ix[k,'revisions_members_2006'] = data[k]['2005-12-31':'2006-12-31'].sum()['revisions']\n",
    "            #number of revisions in 2013\n",
    "            for k in data.keys(): df.ix[k,'revisions_members_2007'] = data[k]['2006-12-31':'2007-12-31'].sum()['revisions']\n",
    "            #number of revisions in 2014\n",
    "            for k in data.keys(): df.ix[k,'revisions_members_2008'] = data[k]['2007-12-31':'2008-12-31'].sum()['revisions']\n",
    "            #number of revisions in 2013\n",
    "            for k in data.keys(): df.ix[k,'revisions_members_2009'] = data[k]['2008-12-31':'2009-12-31'].sum()['revisions']\n",
    "            #number of revisions in 2014\n",
    "            for k in data.keys(): df.ix[k,'revisions_members_2010'] = data[k]['2009-12-31':'2010-12-31'].sum()['revisions']\n",
    "            #number of revisions in 2011\n",
    "            for k in data.keys(): df.ix[k,'revisions_members_2011'] = data[k]['2010-12-31':'2011-12-31'].sum()['revisions']\n",
    "            #number of revisions in 2012\n",
    "            for k in data.keys(): df.ix[k,'revisions_members_2012'] = data[k]['2011-12-31':'2012-12-31'].sum()['revisions']\n",
    "            #number of revisions in 2013\n",
    "            for k in data.keys(): df.ix[k,'revisions_members_2013'] = data[k]['2012-12-31':'2013-12-31'].sum()['revisions']\n",
    "            #number of revisions in 2014\n",
    "            for k in data.keys(): df.ix[k,'revisions_members_2014'] = data[k]['2013-12-31':'2014-12-31'].sum()['revisions']\n",
    "            \n",
    "            \n",
    "            \n",
    "            #number of revisions before 2011\n",
    "            for k in data.keys(): df.ix[k,'revisions_members_bef_2002'] = data[k]['1998-12-31':'2001-12-31'].sum()['revisions']\n",
    "            #number of revisions before 2012\n",
    "            for k in data.keys(): df.ix[k,'revisions_members_bef_2003'] = data[k]['1998-12-31':'2002-12-31'].sum()['revisions']\n",
    "            #number of revisions before 2013\n",
    "            for k in data.keys(): df.ix[k,'revisions_members_bef_2004'] = data[k]['1998-12-31':'2003-12-31'].sum()['revisions']\n",
    "            #number of revisions before 2014\n",
    "            for k in data.keys(): df.ix[k,'revisions_members_bef_2005'] = data[k]['1998-12-31':'2004-12-31'].sum()['revisions']\n",
    "            #number of revisions before 2010\n",
    "            for k in data.keys(): df.ix[k,'revisions_members_bef_2006'] = data[k]['1998-12-31':'2005-12-31'].sum()['revisions']\n",
    "            #number of revisions before 2011\n",
    "            for k in data.keys(): df.ix[k,'revisions_members_bef_2007'] = data[k]['1998-12-31':'2006-12-31'].sum()['revisions']\n",
    "            #number of revisions before 2012\n",
    "            for k in data.keys(): df.ix[k,'revisions_members_bef_2008'] = data[k]['1998-12-31':'2007-12-31'].sum()['revisions']\n",
    "            #number of revisions before 2013\n",
    "            for k in data.keys(): df.ix[k,'revisions_members_bef_2009'] = data[k]['1998-12-31':'2008-12-31'].sum()['revisions']\n",
    "            #number of revisions before 2014\n",
    "            for k in data.keys(): df.ix[k,'revisions_members_bef_2010'] = data[k]['1998-12-31':'2009-12-31'].sum()['revisions']          \n",
    "            #number of revisions before 2010\n",
    "            for k in data.keys(): df.ix[k,'revisions_members_bef_2011'] = data[k]['1998-12-31':'2010-12-31'].sum()['revisions']\n",
    "            #number of revisions before 2011\n",
    "            for k in data.keys(): df.ix[k,'revisions_members_bef_2012'] = data[k]['1998-12-31':'2011-12-31'].sum()['revisions']\n",
    "            #number of revisions before 2012\n",
    "            for k in data.keys(): df.ix[k,'revisions_members_bef_2013'] = data[k]['1998-12-31':'2012-12-31'].sum()['revisions']\n",
    "            #number of revisions before 2013\n",
    "            for k in data.keys(): df.ix[k,'revisions_members_bef_2014'] = data[k]['1998-12-31':'2013-12-31'].sum()['revisions']\n",
    "            #number of revisions before 2014\n",
    "            for k in data.keys(): df.ix[k,'revisions_members_bef_2015'] = data[k]['1998-12-31':'2014-12-31'].sum()['revisions']\n",
    "      \n",
    "    \n",
    "            #nombre de revisions by IP\n",
    "            a,b,c,data = get_pages_ip_revisions_time_series(dataset_dir_name,pages_data.keys() )\n",
    "            #number of revisions in 2011\n",
    "            for k in data.keys(): df.ix[k,'revisions_ip_2001'] = data[k]['2000-12-31':'2001-12-31'].sum()['revisions']\n",
    "            #number of revisions in 2012\n",
    "            for k in data.keys(): df.ix[k,'revisions_ip_2002'] = data[k]['2001-12-31':'2002-12-31'].sum()['revisions']\n",
    "            #number of revisions in 2013\n",
    "            for k in data.keys(): df.ix[k,'revisions_ip_2003'] = data[k]['2002-12-31':'2003-12-31'].sum()['revisions']\n",
    "            #number of revisions in 2014\n",
    "            for k in data.keys(): df.ix[k,'revisions_ip_2004'] = data[k]['2003-12-31':'2004-12-31'].sum()['revisions']\n",
    "            #number of revisions in 2011\n",
    "            for k in data.keys(): df.ix[k,'revisions_ip_2005'] = data[k]['2004-12-31':'2005-12-31'].sum()['revisions']\n",
    "            #number of revisions in 2012\n",
    "            for k in data.keys(): df.ix[k,'revisions_ip_2006'] = data[k]['2005-12-31':'2006-12-31'].sum()['revisions']\n",
    "            #number of revisions in 2013\n",
    "            for k in data.keys(): df.ix[k,'revisions_ip_2007'] = data[k]['2006-12-31':'2007-12-31'].sum()['revisions']\n",
    "            #number of revisions in 2014\n",
    "            for k in data.keys(): df.ix[k,'revisions_ip_2008'] = data[k]['2007-12-31':'2008-12-31'].sum()['revisions']\n",
    "            #number of revisions in 2011\n",
    "            for k in data.keys(): df.ix[k,'revisions_ip_2009'] = data[k]['2008-12-31':'2009-12-31'].sum()['revisions']\n",
    "            #number of revisions in 2012\n",
    "            for k in data.keys(): df.ix[k,'revisions_ip_2010'] = data[k]['2009-12-31':'2010-12-31'].sum()['revisions']\n",
    "            #number of revisions in 2011\n",
    "            for k in data.keys(): df.ix[k,'revisions_ip_2011'] = data[k]['2010-12-31':'2011-12-31'].sum()['revisions']\n",
    "            #number of revisions in 2012\n",
    "            for k in data.keys(): df.ix[k,'revisions_ip_2012'] = data[k]['2011-12-31':'2012-12-31'].sum()['revisions']\n",
    "            #number of revisions in 2013\n",
    "            for k in data.keys(): df.ix[k,'revisions_ip_2013'] = data[k]['2012-12-31':'2013-12-31'].sum()['revisions']\n",
    "            #number of revisions in 2014\n",
    "            for k in data.keys(): df.ix[k,'revisions_ip_2014'] = data[k]['2013-12-31':'2014-12-31'].sum()['revisions']\n",
    "            \n",
    "            \n",
    "            #number of revisions before 2010\n",
    "            for k in data.keys(): df.ix[k,'revisions_ip_bef_2002'] = data[k]['1998-12-31':'2001-12-31'].sum()['revisions']\n",
    "            #number of revisions before 2011\n",
    "            for k in data.keys(): df.ix[k,'revisions_ip_bef_2003'] = data[k]['1998-12-31':'2002-12-31'].sum()['revisions']\n",
    "            #number of revisions before 2012\n",
    "            for k in data.keys(): df.ix[k,'revisions_ip_bef_2004'] = data[k]['1998-12-31':'2003-12-31'].sum()['revisions']\n",
    "            #number of revisions before 2013\n",
    "            for k in data.keys(): df.ix[k,'revisions_ip_bef_2005'] = data[k]['1998-12-31':'2004-12-31'].sum()['revisions']\n",
    "            #number of revisions before 2014\n",
    "            for k in data.keys(): df.ix[k,'revisions_ip_bef_2006'] = data[k]['1998-12-31':'2005-12-31'].sum()['revisions']\n",
    "            #number of revisions before 2010\n",
    "            for k in data.keys(): df.ix[k,'revisions_ip_bef_2007'] = data[k]['1998-12-31':'2006-12-31'].sum()['revisions']\n",
    "            #number of revisions before 2011\n",
    "            for k in data.keys(): df.ix[k,'revisions_ip_bef_2008'] = data[k]['1998-12-31':'2007-12-31'].sum()['revisions']\n",
    "            #number of revisions before 2012\n",
    "            for k in data.keys(): df.ix[k,'revisions_ip_bef_2009'] = data[k]['1998-12-31':'2008-12-31'].sum()['revisions']\n",
    "            #number of revisions before 2013\n",
    "            for k in data.keys(): df.ix[k,'revisions_ip_bef_2010'] = data[k]['1998-12-31':'2009-12-31'].sum()['revisions']\n",
    "            #number of revisions before 2010\n",
    "            for k in data.keys(): df.ix[k,'revisions_ip_bef_2011'] = data[k]['1998-12-31':'2010-12-31'].sum()['revisions']\n",
    "            #number of revisions before 2011\n",
    "            for k in data.keys(): df.ix[k,'revisions_ip_bef_2012'] = data[k]['1998-12-31':'2011-12-31'].sum()['revisions']\n",
    "            #number of revisions before 2012\n",
    "            for k in data.keys(): df.ix[k,'revisions_ip_bef_2013'] = data[k]['1998-12-31':'2012-12-31'].sum()['revisions']\n",
    "            #number of revisions before 2013\n",
    "            for k in data.keys(): df.ix[k,'revisions_ip_bef_2014'] = data[k]['1998-12-31':'2013-12-31'].sum()['revisions']\n",
    "            #number of revisions before 2014\n",
    "            for k in data.keys(): df.ix[k,'revisions_ip_bef_2015'] = data[k]['1998-12-31':'2014-12-31'].sum()['revisions']\n",
    "\n",
    "\n",
    "            #number of pageviews in 2010\n",
    "            a,b,c,data = get_pages_views_time_series(dataset_dir_name , pages_data.keys() )\n",
    "            #number of pageviews in 2012\n",
    "            for k in data.keys(): df.ix[k,'Page_views_2007'] = data[k]['page_views']['2007-12-31']\n",
    "            #number of pageviews in 2013\n",
    "            for k in data.keys(): df.ix[k,'Page_views_2008'] = data[k]['page_views']['2008-12-31']\n",
    "            #number of pageviews in 2014\n",
    "            for k in data.keys(): df.ix[k,'Page_views_2009'] = data[k]['page_views']['2009-12-31']\n",
    "            #number of pageviews in 2014\n",
    "            for k in data.keys(): df.ix[k,'Page_views_2010'] = data[k]['page_views']['2010-12-31']\n",
    "            #number of pageviews in 2011\n",
    "            for k in data.keys(): df.ix[k,'Page_views_2011'] = data[k]['page_views']['2011-12-31']\n",
    "            #number of pageviews in 2012\n",
    "            for k in data.keys(): df.ix[k,'Page_views_2012'] = data[k]['page_views']['2012-12-31']\n",
    "            #number of pageviews in 2013\n",
    "            for k in data.keys(): df.ix[k,'Page_views_2013'] = data[k]['page_views']['2013-12-31']\n",
    "            #number of pageviews in 2014\n",
    "            for k in data.keys(): df.ix[k,'Page_views_2014'] = data[k]['page_views']['2014-12-31']\n",
    "\n",
    "            for k in pages_data.keys(): \n",
    "                quality,impor,islist=get_quality(k)\n",
    "                df.ix[k,'Quality'] = quality\n",
    "                df.ix[k,'Importance'] = impor\n",
    "                df.ix[k,'Is_list'] = islist\n",
    "\n",
    "            \n",
    "            #date of the first contibutions (in number of days after the start of the wikipedia project)\n",
    "            def numberOfDaysAfter(date):\n",
    "                return( (datetime.datetime.strptime(date,\"%Y-%m-%dT%H:%M:%SZ\")-datetime.datetime.strptime(\"2001-01-15T00:00:00Z\",\"%Y-%m-%dT%H:%M:%SZ\")).days)\n",
    "            data={k:min(map(numberOfDaysAfter,map(lambda x: x['timestamp'],v['revisions'])))  for k,v in pages_data.items()}\n",
    "            for k in pages_data.keys(): df.ix[k,'Date'] = data[k]\n",
    "        \n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Compute pages views time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_pages_views_time_series(dataset_dir_name,list_of_page_names):\n",
    "    pages_views_dir_name = '%s/data/pagesviews/' % dataset_dir_name\n",
    "\n",
    "    time_series_dir_name = '%s/stats/time series/' % dataset_dir_name\n",
    "    if not(os.path.exists(time_series_dir_name)): os.makedirs(time_series_dir_name)    \n",
    "\n",
    "    pages_views_daily_ts={}\n",
    "    pages_views_weekly_ts={}       \n",
    "    pages_views_monthly_ts={}\n",
    "    pages_views_yearly_ts={}\n",
    "    \n",
    "    for page_name in list_of_page_names:\n",
    "        path,hashmd5 = get_path_to_file(page_name)\n",
    "        file_name = '%s/%s/%s.json' % (pages_views_dir_name,path, hashmd5)\n",
    "        if not(os.path.exists( '%s/%s' % (time_series_dir_name,path) )):\n",
    "            os.makedirs( '%s/%s' % (time_series_dir_name,path) )\n",
    "        file_name_daily = '%s/%s/%s.pageviews.daily.csv' % (time_series_dir_name,path,hashmd5)\n",
    "        file_name_weekly = '%s/%s/%s.pageviews.weekly.csv' % (time_series_dir_name,path,hashmd5)\n",
    "        file_name_monthly = '%s/%s/%s.pageviews.monthly.csv' % (time_series_dir_name,path,hashmd5)\n",
    "        file_name_yearly = '%s/%s/%s.pageviews.yearly.csv' % (time_series_dir_name,path,hashmd5)\n",
    "    \n",
    "        if os.path.exists(file_name):\n",
    "            if (os.path.exists(file_name_daily)):\n",
    "                pages_views_daily_ts[page_name] = pd.DataFrame().from_csv(file_name_daily)\n",
    "                pages_views_weekly_ts[page_name] = pd.DataFrame().from_csv(file_name_weekly)\n",
    "                pages_views_monthly_ts[page_name] = pd.DataFrame().from_csv(file_name_monthly)\n",
    "                pages_views_yearly_ts[page_name] = pd.DataFrame().from_csv(file_name_yearly)\n",
    "            else:\n",
    "                data=json.load(open(file_name)).items()\n",
    "                index = []\n",
    "                series = []\n",
    "                for k,v in data:\n",
    "                    try:\n",
    "                        index.append(pd.to_datetime(k, format=\"%Y-%m-%d\"))\n",
    "                        series.append(v)\n",
    "                    except ValueError:\n",
    "                        continue\n",
    "                df = pd.DataFrame(series,index=index,columns=['page_views'])\n",
    "                pages_views_daily_ts[page_name]=df\n",
    "                pages_views_daily_ts[page_name].to_csv(file_name_daily,encoding=\"utf-8\")\n",
    "                pages_views_weekly_ts[page_name] = df.resample('W-MON', how='sum')\n",
    "                pages_views_weekly_ts[page_name].to_csv(file_name_weekly,encoding=\"utf-8\")\n",
    "                pages_views_monthly_ts[page_name] = df.resample('M', how='sum')\n",
    "                pages_views_monthly_ts[page_name].to_csv(file_name_monthly,encoding=\"utf-8\")\n",
    "                pages_views_yearly_ts[page_name] = df.resample('A', how='sum')\n",
    "                pages_views_yearly_ts[page_name].to_csv(file_name_yearly,encoding=\"utf-8\")\n",
    "    return(pages_views_daily_ts,pages_views_weekly_ts,pages_views_monthly_ts,pages_views_yearly_ts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Revisions time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_pages_revisions_time_series_gen(pages_revisions,suffixe,time_series_dir_name):\n",
    "    \n",
    "    revisions_daily_ts={}\n",
    "    revisions_weekly_ts={}       \n",
    "    revisions_monthly_ts={}\n",
    "    revisions_yearly_ts={}   \n",
    "    for page_name in pages_revisions.keys():\n",
    "        path,hashmd5 = get_path_to_file(page_name)\n",
    "        if not(os.path.exists( '%s/%s' % (time_series_dir_name,path) )):\n",
    "            os.makedirs( '%s/%s' % (time_series_dir_name,path) )\n",
    "\n",
    "        file_name_daily = '%s/%s/%s.%s.daily.csv'%(time_series_dir_name,path,suffixe,hashmd5)\n",
    "        file_name_weekly = '%s/%s/%s.%s.weekly.csv'%(time_series_dir_name,path,suffixe,hashmd5)\n",
    "        file_name_monthly = '%s/%s/%s.%s.monthly.csv'%(time_series_dir_name,path,suffixe,hashmd5)\n",
    "        file_name_yearly = '%s/%s/%s.%s.yearly.csv'%(time_series_dir_name,path,suffixe,hashmd5)\n",
    "        \n",
    "        if (os.path.exists(file_name_daily)):\n",
    "            revisions_daily_ts[page_name] = pd.DataFrame().from_csv(file_name_daily)\n",
    "            revisions_weekly_ts[page_name] = pd.DataFrame().from_csv(file_name_weekly)\n",
    "            revisions_monthly_ts[page_name] = pd.DataFrame().from_csv(file_name_monthly)\n",
    "            revisions_yearly_ts[page_name] = pd.DataFrame().from_csv(file_name_yearly)\n",
    "        else:\n",
    "            revisions=pages_revisions[page_name]\n",
    "            \n",
    "            if len(revisions)==0:\n",
    "                #print 'No revisions for %s with suffixe: %s' % (page_name,suffixe)\n",
    "                continue\n",
    "            \n",
    "            df = pd.DataFrame(revisions)\n",
    "            df[\"datetime\"] = df.timestamp.apply(pd.to_datetime)\n",
    "            df[\"day\"] = df.datetime.apply(dt.date.strftime, args=('%Y-%m-%d',))\n",
    "            counts = df.groupby(df[\"day\"])\n",
    "            counts = counts.aggregate(len)\n",
    "            series = counts[\"size\"].tolist()\n",
    "            index = counts.index.map(lambda x: pd.to_datetime(x, format=\"%Y-%m-%d\"))\n",
    "            df = pd.DataFrame(series,columns=['revisions'],index=index)\n",
    "            \n",
    "            revisions_daily_ts[page_name]=df\n",
    "            revisions_daily_ts[page_name]=revisions_daily_ts[page_name].fillna(0)\n",
    "            revisions_daily_ts[page_name].to_csv(file_name_daily,encoding=\"utf-8\")\n",
    "            \n",
    "            revisions_weekly_ts[page_name]=df.resample('W-MON', how='sum')\n",
    "            revisions_weekly_ts[page_name]=revisions_weekly_ts[page_name].fillna(0)           \n",
    "            revisions_weekly_ts[page_name].to_csv(file_name_weekly,encoding=\"utf-8\")\n",
    "\n",
    "            revisions_monthly_ts[page_name]=df.resample('M', how='sum')\n",
    "            revisions_monthly_ts[page_name]=revisions_monthly_ts[page_name].fillna(0)\n",
    "            revisions_monthly_ts[page_name].to_csv(file_name_monthly,encoding=\"utf-8\")\n",
    "            \n",
    "            revisions_yearly_ts[page_name]=df.resample('A', how='sum')\n",
    "            revisions_yearly_ts[page_name]=revisions_yearly_ts[page_name].fillna(0)\n",
    "            revisions_yearly_ts[page_name].to_csv(file_name_yearly,encoding=\"utf-8\")\n",
    "    \n",
    "    return(revisions_daily_ts,revisions_weekly_ts,revisions_monthly_ts,revisions_yearly_ts)\n",
    "\n",
    "\n",
    "## ## ## ## ## ## ## ## ## ##\n",
    "\n",
    "\n",
    "def get_pages_revisions_time_series(dataset_dir_name,list_of_page_names):\n",
    "    time_series_dir_name = '%s/stats/time series/' % dataset_dir_name\n",
    "    if not(os.path.exists(time_series_dir_name)): os.makedirs(time_series_dir_name)    \n",
    "\n",
    "    pages_data = gather_pages_data(dataset_dir_name,list_of_page_names)\n",
    "    \n",
    "    data = {}\n",
    "    for k,v in pages_data.iteritems():\n",
    "            data[k]=v['revisions']\n",
    "    \n",
    "    return get_pages_revisions_time_series_gen(data,'revisions',time_series_dir_name)\n",
    "\n",
    "def get_pages_ip_revisions_time_series(dataset_dir_name,list_of_page_names):\n",
    "    time_series_dir_name = '%s/stats/time series/' % dataset_dir_name\n",
    "    if not(os.path.exists(time_series_dir_name)): os.makedirs(time_series_dir_name)    \n",
    "   \n",
    "    pages_data = gather_pages_data(dataset_dir_name,list_of_page_names)\n",
    "\n",
    "    data = {}\n",
    "    for k,v in pages_data.iteritems():\n",
    "            data[k]=[x for x in v['revisions'] if ('userid' in x and x['userid']==0)]\n",
    "    return get_pages_revisions_time_series_gen(data,'revisions.ip',time_series_dir_name)\n",
    "\n",
    "def get_pages_bot_revisions_time_series(dataset_dir_name,list_of_page_names):\n",
    "    time_series_dir_name = '%s/stats/time series/' % dataset_dir_name\n",
    "    if not(os.path.exists(time_series_dir_name)): os.makedirs(time_series_dir_name)    \n",
    "\n",
    "    pages_data = gather_pages_data(dataset_dir_name,list_of_page_names)\n",
    "\n",
    "    data = {}\n",
    "    for k,v in pages_data.iteritems():\n",
    "        data[k]=[x for x in v['revisions'] if ('user' in x and 'bot' in x['user'].lower())]\n",
    "    \n",
    "    return get_pages_revisions_time_series_gen(data,'revisions.bot',time_series_dir_name)\n",
    "\n",
    "def get_pages_members_revisions_time_series(dataset_dir_name,list_of_page_names):\n",
    "    time_series_dir_name = '%s/stats/time series/' % dataset_dir_name\n",
    "    if not(os.path.exists(time_series_dir_name)): os.makedirs(time_series_dir_name)    \n",
    "\n",
    "    pages_data = gather_pages_data(dataset_dir_name,list_of_page_names)\n",
    "\n",
    "    data = {}\n",
    "    for k,v in pages_data.iteritems():\n",
    "        data[k]=[x for x in v['revisions'] if ('user' in x and x['userid']!=0 and 'bot' not in x['user'].lower())]\n",
    "    \n",
    "    return get_pages_revisions_time_series_gen(data,'revisions.members',time_series_dir_name)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Gather and compute data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def gather_and_compute_data(dataset_dir_name,list_of_page_names):\n",
    "    gather_pages_data(dataset_dir_name, list_of_page_names)\n",
    "    gather_pages_views(dataset_dir_name,list_of_page_names)\n",
    "    get_pages_views_time_series(dataset_dir_name,list_of_page_names)\n",
    "    get_pages_members_revisions_time_series(dataset_dir_name,list_of_page_names)\n",
    "    get_pages_bot_revisions_time_series(dataset_dir_name,list_of_page_names)\n",
    "    get_pages_ip_revisions_time_series(dataset_dir_name,list_of_page_names)\n",
    "    get_pages_revisions_time_series(dataset_dir_name,list_of_page_names) \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Statistics computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#Report for evolution of revisions and pageviews for a set of pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "def compute_sum_ts_weekly(dataset_dir_name,list_of_page_names,function,col_name):\n",
    "    df = pd.DataFrame()\n",
    "    time_series_dir_name = '%s/stats/time series/' % dataset_dir_name\n",
    "    for title in list_of_page_names:\n",
    "        try:\n",
    "            a,dic_ts,c,d = function(dataset_dir_name,[title])\n",
    "            if title in dic_ts.keys():\n",
    "                if (len(df.columns)==0):\n",
    "                    df = dic_ts[title].copy()\n",
    "                else:\n",
    "                    tmp = dic_ts[title].copy()\n",
    "                    tmp.columns = [x.replace(col_name,'tmp') for x in tmp.columns]\n",
    "                    df = df.join(tmp,how='outer').fillna(0)\n",
    "                    df['sum'] = df[col_name] + df['tmp']\n",
    "                    df.drop(col_name, axis=1, inplace=True)\n",
    "                    df.drop('tmp', axis=1, inplace=True)\n",
    "                    df.columns = [col_name]\n",
    "            else:\n",
    "                pass\n",
    "        except:\n",
    "            a,dic_ts,c,d = function(dataset_dir_name,[title])\n",
    "            print dic_ts\n",
    "            return(df)\n",
    "    return(df)\n",
    "\n",
    "        \n",
    "        \n",
    "def get_report_set_of_pages(dataset_dir_name,list_of_page_names,ip=True,bot=True,member=True):\n",
    "    display(HTML(\"<h2>Evolution of pageviews and revisions</h2>\" ))\n",
    "   \n",
    "    df_pageviews = compute_sum_ts_weekly(dataset_dir_name,list_of_page_names,get_pages_views_time_series,'page_views')\n",
    "    \n",
    "    \n",
    "\n",
    "    if ip:\n",
    "        display(HTML(\"<h3>Evolution of pageviews and revisions by ip</h3>\" ))\n",
    "        df_ip = compute_sum_ts_weekly(dataset_dir_name,list_of_page_names,get_pages_ip_revisions_time_series,'revisions').join(df_pageviews,how='outer').fillna(0)  \n",
    "\n",
    "        df_ip['revisions'].plot(figsize=(11,4), linewidth=\"0.5\", ylim=0, colormap=\"Spectral\", rot=0)\n",
    "        df_ip['page_views'].plot(secondary_y=True, style=\"-\", linewidth=\"0.5\", ylim=0,sharex=True)\n",
    "        plt.show()\n",
    "        \n",
    "    if member:      \n",
    "        display(HTML(\"<h3>Evolution of pageviews and revisions by members</h3>\" ))\n",
    "        df_members = compute_sum_ts_weekly(dataset_dir_name,list_of_page_names,get_pages_members_revisions_time_series,'revisions').join(df_pageviews,how='outer').fillna(0)\n",
    "        df_members['revisions'].plot(figsize=(11,4), linewidth=\"0.5\", ylim=0, colormap=\"Spectral\", rot=0)\n",
    "        df_members['page_views'].plot(secondary_y=True, style=\"-\", linewidth=\"0.5\", ylim=0,sharex=True)\n",
    "        plt.show()\n",
    "\n",
    "    if bot:\n",
    "        display(HTML(\"<h3>Evolution of pageviews and revisions by bot</h2>\" ))\n",
    "        df_bot = compute_sum_ts_weekly(dataset_dir_name,list_of_page_names,get_pages_bot_revisions_time_series,'revisions').join(df_pageviews,how='outer').fillna(0)\n",
    "        df_bot['revisions'].plot(figsize=(11,4), linewidth=\"0.5\", ylim=0, colormap=\"Spectral\", rot=0)\n",
    "        df_bot['page_views'].plot(secondary_y=True, style=\"-\", linewidth=\"0.5\", ylim=0,sharex=True)\n",
    "        plt.show()\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_sum_ts_monthly(dataset_dir_name,list_of_page_names,function,col_name):\n",
    "    df = pd.DataFrame()\n",
    "    time_series_dir_name = '%s/stats/time series/' % dataset_dir_name\n",
    "    \n",
    "    for title in list_of_page_names:\n",
    "        a,b,dic_ts,d = function(dataset_dir_name,[title])\n",
    "        if title in dic_ts.keys():\n",
    "            if (len(df.columns)==0):\n",
    "                df = dic_ts[title].copy()\n",
    "            else:\n",
    "                tmp = dic_ts[title].copy()\n",
    "                tmp.columns = [x.replace(col_name,'tmp') for x in tmp.columns]\n",
    "                df = df.join(tmp,how='outer').fillna(0)\n",
    "                df['sum'] = df[col_name] + df['tmp']\n",
    "                df.drop(col_name, axis=1, inplace=True)\n",
    "                df.drop('tmp', axis=1, inplace=True)\n",
    "                df.columns = [col_name]\n",
    "        else:\n",
    "            pass\n",
    "    return(df)\n",
    "\n",
    "        \n",
    "        \n",
    "def get_monthly_report_set_of_pages(dataset_dir_name,list_of_page_names,ip=True,bot=True,member=True):\n",
    "    display(HTML(\"<h2>Evolution of pageviews and revisions</h2>\" ))\n",
    "   \n",
    "    df_pageviews = compute_sum_ts_monthly(dataset_dir_name,list_of_page_names,get_pages_views_time_series,'page_views')\n",
    "    \n",
    "    if ip:\n",
    "        display(HTML(\"<h3>Evolution of pageviews and revisions by ip</h3>\" ))\n",
    "        df_ip = compute_sum_ts_monthly(dataset_dir_name,list_of_page_names,get_pages_ip_revisions_time_series,'revisions').join(df_pageviews,how='outer').fillna(0)  \n",
    "\n",
    "        df_ip['revisions'].plot(figsize=(11,4), linewidth=\"0.5\", ylim=0, colormap=\"Spectral\", rot=0)\n",
    "        df_ip['page_views'].plot(secondary_y=True, style=\"-\", linewidth=\"0.5\", ylim=0,sharex=True)\n",
    "        plt.show()\n",
    "        \n",
    "    if member:      \n",
    "        display(HTML(\"<h3>Evolution of pageviews and revisions by members</h3>\" ))\n",
    "        df_members = compute_sum_ts_monthly(dataset_dir_name,list_of_page_names,get_pages_members_revisions_time_series,'revisions').join(df_pageviews,how='outer').fillna(0)\n",
    "\n",
    "        df_members['revisions'].plot(figsize=(11,4), linewidth=\"0.5\", ylim=0, colormap=\"Spectral\", rot=0)\n",
    "        df_members['page_views'].plot(secondary_y=True, style=\"-\", linewidth=\"0.5\", ylim=0,sharex=True)\n",
    "        plt.show()\n",
    "\n",
    "    if bot:\n",
    "        display(HTML(\"<h3>Evolution of pageviews and revisions by bot</h2>\" ))\n",
    "        df_bot = compute_sum_ts_monthly(dataset_dir_name,list_of_page_names,get_pages_bot_revisions_time_series,'revisions').join(df_pageviews,how='outer').fillna(0)\n",
    "\n",
    "        df_bot['revisions'].plot(figsize=(11,4), linewidth=\"0.5\", ylim=0, colormap=\"Spectral\", rot=0)\n",
    "        df_bot['page_views'].plot(secondary_y=True, style=\"-\", linewidth=\"0.5\", ylim=0,sharex=True)\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_sum_ts_yearly(dataset_dir_name,list_of_page_names,function,col_name):\n",
    "    df = pd.DataFrame()\n",
    "    time_series_dir_name = '%s/stats/time series/' % dataset_dir_name\n",
    "    \n",
    "    for title in list_of_page_names:\n",
    "        a,b,c,dic_ts = function(dataset_dir_name,[title])\n",
    "        if title in dic_ts.keys():\n",
    "            if (len(df.columns)==0):\n",
    "                df = dic_ts[title].copy()\n",
    "            else:\n",
    "                tmp = dic_ts[title].copy()\n",
    "                tmp.columns = [x.replace(col_name,'tmp') for x in tmp.columns]\n",
    "                df = df.join(tmp,how='outer').fillna(0)\n",
    "                df['sum'] = df[col_name] + df['tmp']\n",
    "                df.drop(col_name, axis=1, inplace=True)\n",
    "                df.drop('tmp', axis=1, inplace=True)\n",
    "                df.columns = [col_name]\n",
    "        else:\n",
    "            pass\n",
    "    return(df)\n",
    "\n",
    "        \n",
    "        \n",
    "def get_yearly_report_set_of_pages(dataset_dir_name,list_of_page_names,ip=True,bot=True,member=True):\n",
    "    display(HTML(\"<h2>Evolution of pageviews and revisions</h2>\" ))\n",
    "   \n",
    "    df_pageviews = compute_sum_ts_yearly(dataset_dir_name,list_of_page_names,get_pages_views_time_series,'page_views')\n",
    "    \n",
    "    if ip:\n",
    "        display(HTML(\"<h3>Evolution of pageviews and revisions by ip</h3>\" ))\n",
    "        df_ip = compute_sum_ts_yearly(dataset_dir_name,list_of_page_names,get_pages_ip_revisions_time_series,'revisions').join(df_pageviews,how='outer').fillna(0)  \n",
    "\n",
    "        df_ip['revisions'].plot(figsize=(11,4), linewidth=\"0.5\", ylim=0, colormap=\"Spectral\", rot=0)\n",
    "        df_ip['page_views'].plot(secondary_y=True, style=\"-\", linewidth=\"0.5\", ylim=0,sharex=True)\n",
    "        plt.show()\n",
    "        \n",
    "    if member:      \n",
    "        display(HTML(\"<h3>Evolution of pageviews and revisions by members</h3>\" ))\n",
    "        df_members = compute_sum_ts_yearly(dataset_dir_name,list_of_page_names,get_pages_members_revisions_time_series,'revisions').join(df_pageviews,how='outer').fillna(0)\n",
    "\n",
    "        df_members['revisions'].plot(figsize=(11,4), linewidth=\"0.5\", ylim=0, colormap=\"Spectral\", rot=0)\n",
    "        df_members['page_views'].plot(secondary_y=True, style=\"-\", linewidth=\"0.5\", ylim=0,sharex=True)\n",
    "        plt.show()\n",
    "\n",
    "    if bot:\n",
    "        display(HTML(\"<h3>Evolution of pageviews and revisions by bot</h2>\" ))\n",
    "        df_bot = compute_sum_ts_yearly(dataset_dir_name,list_of_page_names,get_pages_bot_revisions_time_series,'revisions').join(df_pageviews,how='outer').fillna(0)\n",
    "\n",
    "        df_bot['revisions'].plot(figsize=(11,4), linewidth=\"0.5\", ylim=0, colormap=\"Spectral\", rot=0)\n",
    "        df_bot['page_views'].plot(secondary_y=True, style=\"-\", linewidth=\"0.5\", ylim=0,sharex=True)\n",
    "        plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
